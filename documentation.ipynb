{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Report: Fine-Tuning BERT for Multilabel Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "The objective of this project is to fine-tune a BERT[`bert-base-uncased`][`110M prams`] model for multilabel text classification.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Methodology\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Dataset Details\n",
    "\n",
    "We utilized the SemEval 2018 Task 1 dataset, specifically focusing on Subtask 5 for English language text. The dataset contains tweets with associated multilabel classifications. Labels include various sentiment and emotion categories.\n",
    "##### Example of dataset\n",
    "```yaml\n",
    "{'ID': '2017-En-21441',\n",
    " 'Tweet': \"â€œWorry is a down payment on a problem you may never have'. \\xa0Joyce Meyer.  #motivation #leadership #worry\",\n",
    " 'anger': False,\n",
    " 'anticipation': True,\n",
    " 'disgust': False,\n",
    " 'fear': False,\n",
    " 'joy': False,\n",
    " 'love': False,\n",
    " 'optimism': True,\n",
    " 'pessimism': False,\n",
    " 'sadness': False,\n",
    " 'surprise': False,\n",
    " 'trust': True}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Model Architecture\n",
    "\n",
    "We employed the BERT-base-uncased pre-trained model as the backbone for our multilabel text classification task. The model was configured for multi-label classification, with the number of labels corresponding to the available categories in the dataset.\n",
    "\n",
    "### 2.3 Preprocessing Data\n",
    "\n",
    "Text data underwent tokenization and encoding using the BERT tokenizer, with padding to a maximum length of 128 tokens. Labels were processed to create a binary matrix representing the presence or absence of each label for each example.\n",
    "\n",
    "### 2.4 Training Process\n",
    "\n",
    "The model was trained using the Hugging Face `Trainer` class. Training parameters included a batch size of 8, a learning rate of 2e-5, and training for 5 epochs. The F1 score, ROC AUC, and accuracy metrics were used to evaluate model performance.\n",
    "\n",
    "### 2.5 Evaluation Metrics\n",
    "\n",
    "We utilized a custom evaluation metric function that computed F1 score, ROC AUC score, and accuracy for multilabel classification. This function was employed during training to monitor and assess model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Results\n",
    "\n",
    "### 3.1 Training Results\n",
    "\n",
    "The training process yielded promising results, with the model achieving high F1 scores, ROC AUC scores, and accuracy on the validation set. Detailed metrics for each epoch are provided in the training logs.\n",
    "\n",
    "### 3.2 Inference Results\n",
    "\n",
    "Inference on sample text demonstrated the model's ability to make accurate multilabel predictions. The threshold for label assignment was set at 0.5, and the predicted labels were extracted based on this threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Challenges Faced\n",
    "\n",
    "### 4.1 GPU Resource Limitations\n",
    "\n",
    "One major challenge was the limited availability of high-computation GPUs. To overcome this, I started with smaller model 'bert-based-uncased''110M prams' and explored quantized models to reduce resource requirements.\n",
    "\n",
    "### 4.2 Fine-Tuning Process\n",
    "\n",
    "Fine-tuning BERT for multilabel classification required careful consideration of label processing and model configuration. The custom evaluation metric function was crucial for assessing model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusions\n",
    "\n",
    "The fine-tuned BERT model demonstrated strong performance on multilabel text classification tasks. Hugging face trainer API was used to finetune the dataset for multilabel text classification. The challenges related to GPU resources were mitigated by exploring alternative model configurations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Future Work\n",
    "\n",
    "Future work may involve experimenting with larger BERT models and other LLMs, exploring other datasets, and conducting a more extensive evaluation against existing multilabel classification models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
